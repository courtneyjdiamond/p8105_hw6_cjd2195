---
title: "P8105 HW6"
author: "Courtney Diamond"
date: "2023-11-28"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)

knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Time to load some data. 

```{r}
homicide_data = 
  read_csv("data/homicide-data.csv") |> 
  mutate(city_state = str_c(as.character(city), as.character(state), sep = ", ")) |> 
  filter(!city_state %in% c('Tulsa, AL', 'Dallas, TX', 'Phoenix, AZ', 'Kansas City, MO')) |> 
  filter(victim_race %in% c('White', 'Black')) |> 
  mutate(victim_age = as.numeric(victim_age)) |> 
  mutate(status = as.numeric(disposition == 'Closed by arrest'))

homicide_data
```

First let's narrow down to Baltimore, MD
```{r}
baltimore_homicide_df = 
  homicide_data |> 
  filter(city_state == 'Baltimore, MD')

baltimore_homicide_df
```

Now let's run the glm! 

```{r}
baltimore_fit = 
  baltimore_homicide_df |> 
  glm(status ~ victim_age + victim_race + victim_sex, data = _, family = binomial()) |> 
  broom::tidy() |> 
  mutate(OR = exp(estimate),
         OR_CI_upper = exp(estimate + 1.96 * std.error),
         OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  select(term, OR, OR_CI_upper, OR_CI_lower, p.value) |> 
  filter(term == 'victim_sexMale') |> 
  knitr::kable()

  
baltimore_fit
```

Keeping all other variables fixed, the odds of a homicide being solved for male victims are lower than for female victims. 

Now, let's transform this to a pipeline to do the same sort of calculation for all the cities in the original dataset. 

```{r}
male_odds_solved =   
  homicide_data |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(status ~ victim_age + victim_race + victim_sex, data = df, family = binomial())),
    ) |> 
  mutate(
    tidy_models = map(models, broom::tidy)
    ) |> 
  unnest(tidy_models) |> 
  mutate(
    OR = exp(estimate),
    upper_OR_CI = exp(estimate + 1.96 * std.error),
    lower_OR_CI = exp(estimate - 1.96 * std.error)
  ) |> 
  select(city_state, term, OR, upper_OR_CI, lower_OR_CI, p.value) |> 
  filter(term == 'victim_sexMale')

male_odds_solved |> 
  knitr::kable(digits = 3)
  
```


```{r}
male_odds_solved |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = lower_OR_CI, ymax = upper_OR_CI)) + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Most of the cities plotted have ORs less than 1 after adjusting for race and age, indicating smaller odds of a male victim's homicide being solved compared to females'. Nashville, TN, Fresno, CA, Stockton, CA, and Albuquerque, NM have ORs greater than 1. 22 of the cities have confidence intervals that do not include 1, indicating a statistically significant result. 


## Problem 2

First let's load the data. 
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())

weather_df
```

Now let's make a quick function to actually take a single bootstrap sample. 

```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE)
  
}
```

Great, now let's build a pipeline to do this a few (thousand) times. We'll examine one of the strapped samples just to be safe. 


```{r}
boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(weather_df))
  )

boot_straps |> 
  pull(strap_sample) |> 
  nth(1) |> 
  arrange(date) |> 
  head(n = 25) |> 
  knitr::kable()
```

Cool, for the first bootstrapped sample, even just in the first few lines, we can see that there are many dates which were pulled more than once (e.g. January 2, January 3, etc. etc.)

Now let's get estimates for these two quantities for each of our bootstrap samples. 

```{r}
boot_results = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  select(strap_number, models, results) |> 
  unnest(results) |> 
  select(strap_number, models, term, estimate, std.error) |>  
  mutate(
    r2 = map(models, broom::glance)
  ) |> 
  unnest(r2)

compact_boot_results =
  boot_results |> 
  select(strap_number, models, term, estimate, r.squared) |> 
  pivot_wider(names_from = term,
              values_from = estimate) |> 
  rename(beta_0 = "(Intercept)",
         beta_1 = "tmin",
         beta_2 = "prcp") |> 
  mutate(
    log_b1_b2 = log(beta_1 * beta_2)
  )
  
compact_boot_results
  
```

Hooray! Let's plot them. It's important to note that, because many of the $\hat{\beta_2}$ values calculated are negative, the $log(\hat{\beta_1} * \hat{\beta_2})$ value is going to be `NaN` for these samples, and thus will be dropped when plotting the distribution.

```{r}
compact_boot_results |> 
  ggplot(aes(x = r.squared)) + 
  geom_density()
```

The distribution of the $\hat{r^2}$ values is negatively skewed (i.e. the mean is less than the median value), with the mode existing around 0.92 (ish, I am eyeballing it.)

```{r}
compact_boot_results |> 
  ggplot(aes(x = log_b1_b2)) +
  geom_density()
```

The distribution of the $log(\hat{\beta_1} * \hat{\beta_2})$ values is also negatively skewed (i.e. the mean is less than the median value), with the mode existing around -5.5 (ish, I am eyeballing it.)

```{r}
compact_boot_results |> 
  summarize(
    rsq_ci_lower = quantile(r.squared, 0.025),
    rsq_ci_upper = quantile(r.squared, 0.975),
    log_beta_ci_lower = quantile(log_b1_b2, 0.025, na.rm = TRUE),
    log_beta_ci_upper = quantile(log_b1_b2, 0.975, na.rm = TRUE)
  ) |> 
  knitr::kable(digits = 3)
```

The estimate of $\hat{r^2}$ is `r compact_boot_results |> summarize(mean(r.squared))`, 95% CI [`r compact_boot_results |>  summarize(quantile(r.squared, 0.025))`, `r compact_boot_results |>  summarize(quantile(r.squared, 0.975))`]. The estimate of $log(\hat{\beta_1} * \hat{\beta_2})$ is `r compact_boot_results |> summarize(mean(log_b1_b2, na.rm = TRUE))`, 95% CI [`r compact_boot_results |>  summarize(quantile(log_b1_b2, 0.025, na.rm = TRUE))`, `r compact_boot_results |>  summarize(quantile(log_b1_b2, 0.975, na.rm = TRUE))`]. As above, it's important to note that because the value of $log(\hat{\beta_1} * \hat{\beta_2})$ is the result of trying to take the logarithm of a negative number, these are `NaN` and are not included when calculating the estimate and confidence interval. 

## Problem 3

Let's read in some new data!

```{r}

birthweight_df = 
  read_csv("data/birthweight.csv") |> 
  mutate(
    babysex = case_match(
    babysex,
    1 ~ "male",
    2 ~ "female"
  ),
  babysex = as.factor(babysex),
  frace = case_match(
    frace,
    1 ~ "white",
    2 ~ "black",
    3 ~ "asian",
    4 ~ "puerto rican",
    8 ~ "other",
    9 ~ "unknown"
  ),
  frace = as.factor(frace),
  malform = case_match(
    malform, 
    0 ~ "absent",
    1 ~ "present"
  ),
  malform = as.factor(malform),
  mrace = case_match(
    mrace,
    1 ~ "white",
    2 ~ "black",
    3 ~ "asian",
    4 ~ "puerto rican",
    8 ~ "other",
  ),
  mrace = as.factor(mrace), 
  bwt_lbs = bwt* 0.00225,
  mheight_cm = mheight * 2.54,
  fincome_full = fincome * 100)


birthweight_df |> 
  summarise(across(everything(), ~ sum(is.na(.x))))

birthweight_df
```

